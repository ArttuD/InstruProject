{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\srboval1\\AppData\\Local\\anaconda3\\envs\\stan\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import arviz as az\n",
    "from cmdstanpy import cmdstan_path, CmdStanModel\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import seaborn as sns\n",
    "import os\n",
    "from IPython.display import display, HTML\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data parser\n",
    "\n",
    "Find and download all microrheology data and format dictionary for stan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:/Users/srboval1/IPN15/AlgVLVG15_3Ca\\\\230728_3_10_plain\\\\results\\\\summary_ref_level.csv', 'C:/Users/srboval1/IPN15/AlgVLVG15_3Ca\\\\230814_3_10_plain\\\\results\\\\summary_ref_level.csv', 'C:/Users/srboval1/IPN15/AlgVLVG15_3Ca\\\\230815_3_10_plain\\\\results\\\\summary_ref_level.csv']\n",
      "        day  frequency concentration      type  sample  holder  location  \\\n",
      "0    230728       0.05           3/2  IPN_soft       1       1         1   \n",
      "1    230728       0.05           3/2  IPN_soft       1       1         1   \n",
      "2    230728       0.05           3/2  IPN_soft       1       1         1   \n",
      "3    230728       0.05           3/2  IPN_soft       1       1         1   \n",
      "4    230728       0.05           3/2  IPN_soft       1       1         1   \n",
      "..      ...        ...           ...       ...     ...     ...       ...   \n",
      "169  230815       0.05           3/2  IPN_soft       1       3         3   \n",
      "170  230815       0.05           3/2  IPN_soft       1       3         3   \n",
      "171  230815       0.05           3/2  IPN_soft       1       3         3   \n",
      "172  230815       0.05           3/2  IPN_soft       1       3         3   \n",
      "173  230815       0.05           3/2  IPN_soft       1       3         3   \n",
      "\n",
      "     repeat  track_id  reference_id  ...   inv.rmse  shift_(s)   a_error  \\\n",
      "0         1         0             6  ...  27.375503      1.725  0.001564   \n",
      "1         1         0             8  ...  35.054848      1.725  0.001221   \n",
      "2         1         0             9  ...  32.851676      1.725  0.001303   \n",
      "3         1         0            11  ...  20.884387      1.725  0.002050   \n",
      "4         1         1            11  ...  41.463305      1.725  0.001033   \n",
      "..      ...       ...           ...  ...        ...        ...       ...   \n",
      "169       1         2             2  ...  11.653367      1.899  0.003624   \n",
      "170       1         2             3  ...   8.258653      1.899  0.005191   \n",
      "171       1         4             2  ...  10.002012      1.899  0.004283   \n",
      "172       1         6             4  ...  46.874667      1.899  0.000915   \n",
      "173       1         8             1  ...  51.156005      1.899  0.000826   \n",
      "\n",
      "     phi_error   c_error   d_error           x           y  phi_(deg)  \\\n",
      "0     0.003834  0.002148  0.000123  331.968060  398.107371   9.488163   \n",
      "1     0.003156  0.001677  0.000096  331.968060  398.107371   9.843113   \n",
      "2     0.005541  0.001790  0.000102  331.968060  398.107371  11.316208   \n",
      "3     0.004657  0.002816  0.000161  331.968060  398.107371   9.739070   \n",
      "4     0.005393  0.001418  0.000081  339.828324  553.043164   9.131057   \n",
      "..         ...       ...       ...         ...         ...        ...   \n",
      "169   0.024607  0.005045  0.000289  329.549296  637.192889  37.556207   \n",
      "170   0.015313  0.007118  0.000407  329.549296  637.192889  -0.450000   \n",
      "171   0.026159  0.005877  0.000336  567.759815  337.460516   7.292121   \n",
      "172   0.018253  0.001254  0.000072  567.949847  219.815769  -0.450000   \n",
      "173   0.012053  0.001149  0.000066  294.463621  200.704922  36.499122   \n",
      "\n",
      "      tan_phi  \n",
      "0    0.167130  \n",
      "1    0.173505  \n",
      "2    0.200114  \n",
      "3    0.171635  \n",
      "4    0.160730  \n",
      "..        ...  \n",
      "169  0.768887  \n",
      "170 -0.007854  \n",
      "171  0.127963  \n",
      "172 -0.007854  \n",
      "173  0.739937  \n",
      "\n",
      "[1553 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "# Download data and make a mega .csv fiel\n",
    "\n",
    "paths=[]\n",
    "paths = glob.glob(os.path.join('C:/Users/srboval1/IPN15/AlgVLVG15_3Ca/*','**','summary_ref_level.csv'))\n",
    "print(paths)\n",
    "concentration = '3/2'\n",
    "type ='IPN_soft'\n",
    "\n",
    "a = []\n",
    "for i in paths:\n",
    "    path1 = os.path.split(os.path.split(i)[0])[0]\n",
    "    path1.replace(\"/\",\"\\\\\")\n",
    "    splitted= path1.split('\\\\')[-1]\n",
    "    tmp = pd.read_csv(i)\n",
    "    tmp['coating_type'] = splitted.split('_')[-1]\n",
    "    tmp['size'] = splitted.split('_')[-2]\n",
    "    tmp['day'] = splitted.split('_')[-4]\n",
    "    a.append(tmp)\n",
    "\n",
    "a_concantenated=[]\n",
    "a_concantenated = pd.concat(a)\n",
    "a_concantenated['radius_(m)'] *= 1e6\n",
    "a_concantenated = a_concantenated.rename(columns={'radius_(m)':'radius_(um)'})\n",
    "a_concantenated['concentration'] = concentration\n",
    "a_concantenated['type'] = type\n",
    "a_concantenated['frequency'] = 0.05\n",
    "a_concantenated = a_concantenated.reindex(columns=['day','frequency','concentration','type','sample','holder','location','repeat','track_id','reference_id','distance(um)','Cov_Sum','a_(um)','phi_(rad)','c','d','G_abs','radius_(m)','r2','rmse','inv.rmse','shift_(s)','a_error','phi_error','c_error','d_error','x','y','phi_(deg)','tan_phi'])\n",
    "print(a_concantenated)\n",
    "#%%\n",
    "if os.path.exists('C:/Users/srboval1/IPN15/IPN15.csv'):\n",
    "    a_new = pd.DataFrame(a_concantenated)\n",
    "    a_new.to_csv('C:/Users/srboval1/IPN15/IPN15.csv', mode='a', index=False, header=False)\n",
    "else:\n",
    "    a_concantenated.to_csv(\"C:/Users/srboval1/IPN15/IPN15.csv\", index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "687.3652925606983\n",
      "8.000944348706229\n",
      "['G_abs', 'phi_(rad)'] \n",
      " ['collagen' 'IPN_tuned']\n"
     ]
    }
   ],
   "source": [
    "data_all = pd.read_csv('C:/Users/srboval1/IPN15/datas.csv')\n",
    "\n",
    "#calculating G std for each material typeand then choosing the highest one of all\n",
    "G_std = np.max(data_all[['type','day','holder','location','track_id',\"G_abs\", \"phi_(deg)\"]].groupby([\"type\"]).std()[\"G_abs\"].values)\n",
    "print(G_std)\n",
    "\n",
    "#calculating G std for each material type and then choosing the highest one of all\n",
    "phi_std = np.max(data_all[['type','day','holder','location','track_id',\"G_abs\", \"phi_(deg)\"]].groupby([\"type\"]).std()[\"phi_(deg)\"].values)\n",
    "print(phi_std)\n",
    "\n",
    "\n",
    "full_path = os.path.split(os.getcwd())[0]\n",
    "\n",
    "print([\"G_abs\", \"phi_(rad)\"], \"\\n\",data_all[\"type\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This python code which includes model definition, which itself is written in Stan language, while Python is used to preprocess data, compile the model, run inference, and analyze results.\n",
    "\n",
    "1. Creating a dictionary from the collected data - preprocessing\n",
    "2. Compiling the model, sampling it on the data\n",
    "3. Interference and analysis - fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function preprocessing data and fitting model\n",
    "#model_name: name of the stan model e.g hier_material (no .stan)\n",
    "#diagnoise: print mcmc diagnostics, rhat...\n",
    "#returns: tuple of two dicts where keys are G_abs and phi_(deg)\n",
    "#first containts the mcmc samples as pandas dataframe\n",
    "#seconds contains arviz datastructure for visualizations\n",
    "def preprocessing(model_name, diagnose = False):\n",
    "        \n",
    "    out = {}\n",
    "    az_out = {}\n",
    "\n",
    "    data_all = pd.read_csv('C:/Users/srboval1/IPN15/datas.csv')\n",
    "\n",
    "    #process G and phi measurements separately, \n",
    "    for measurement in ['G_abs','phi_(deg)']:\n",
    "        samples = []\n",
    "        out[measurement] = {}\n",
    "\n",
    "        #combine information of all materials\n",
    "        #looping through each material one by one\n",
    "        for material in data_all[\"type\"].unique():\n",
    "            data_type = material\n",
    "            data = data_all[data_all['type'] == data_type]\n",
    "\n",
    "            #initializing empty list that will contain corresponding numbers for each sample\n",
    "            sample_num = []\n",
    "        \n",
    "            prev = None\n",
    "            idx = 1\n",
    "            #assuming one sample per day, adding sample ids to the list in sample_num\n",
    "            #iterating over each element in the 'day' column of data and creating the sample_num list\n",
    "            for i in data['day']:\n",
    "                if prev is None:\n",
    "                    prev = i\n",
    "                else: #changed from Ossi's code 'if' - to have distinct sample ids\n",
    "                    idx += 1\n",
    "                    prev = i\n",
    "                sample_num.append(idx)      \n",
    "            \n",
    "            data['sample_num'] = sample_num\n",
    "            subset = data [['sample_num','holder','location', 'track_id', measurement]]\n",
    "            subset['sample_num']=subset['sample_num'].astype(int) #convertin sample numbers to integers\n",
    "            \n",
    "            sample_ids=[]\n",
    "            location_ids=[]\n",
    "            holder_ids=[]\n",
    "            track_ids=[]\n",
    "\n",
    "            n_loc = 0       #initial number of ids in locations\n",
    "            n_hold = 0      #initial locations in holders\n",
    "            n_samp = 0    #initial holders in samples\n",
    "\n",
    "        #change into consecutive numbering\n",
    "            for i in subset['sample_num'].unique():             \n",
    "                sub_sample = subset[subset['sample_num']==i]        #new df containing only the rows (measurements) with that unique sample_num i\n",
    "                sample_ids.extend([int(i)]*sub_sample['holder'].unique().shape[0])\n",
    "                for jj,j in enumerate(sub_sample['holder'].unique()):\n",
    "                    sub = sub_sample[sub_sample['holder']==j]\n",
    "                    holder_ids.extend([int(jj+1)+n_samp]*sub['location'].unique().shape[0]) #list that will eventually be listing all holder ids for each location - add identity number of holder times the number of locations (unique holder numbers)\n",
    "                    for kk,k in enumerate(sub['location'].unique()):\n",
    "                        sub2 = sub[sub['location']==k]\n",
    "                        location_ids.extend([int(kk+1)+n_hold]*sub2['track_id'].unique().shape[0])\n",
    "                        for zz, z in enumerate(sub2['track_id'].unique()):\n",
    "                            sub3 = sub2[sub2['track_id']==z]\n",
    "                            track_ids.extend([int(zz+1)+n_loc]*sub3.shape[0]) #shape[0] size of the first dimension = row\n",
    "                        n_loc += int(sub2['track_id'].unique().shape[0])    #number of tracks ids that have been looped through already; updated with each location\n",
    "                    n_hold += int(sub['location'].unique().shape[0])    #number of locations that have been looped through; used to add up in the next holder\n",
    "                n_samp += int(sub_sample['holder'].unique().shape[0])   #number of holders that have been looped through; used to add up in the next sample\n",
    "\n",
    "        # normalize mean to zero and with common std \n",
    "        y_raw = subset[measurement].values  #retrieves values of the column labelled measurement from df subset and converts into array by .value\n",
    "        if measurement == 'G_abs':          #compares measurement to a fixed string 'G_abs' variable\n",
    "            y = y_raw/G_std                 #each value scaled by std\n",
    "        else:\n",
    "            y = y_raw/phi_std\n",
    "\n",
    "        #defining dictionary; curly brackets and specified key-value pairs separated by : each key corresponds to a specific attribute }\n",
    "        sample = {      'N'         :subset.shape[0],\n",
    "                        'N_samples' : int(np.max(sample_ids)),\n",
    "                        'N_holders' : int(np.max(holder_ids)),\n",
    "                        'N_locations':int(np.max(location_ids)),\n",
    "                        'N_ids'     : int(np.max(track_ids)),\n",
    "                        'sample_ids': sample_ids,\n",
    "                        'holder_ids': holder_ids,\n",
    "                        'location_ids':location_ids,\n",
    "                        'track_ids':  track_ids,\n",
    "                        'train_ids':  (np.arange(len(track_ids))+1).tolist(),\n",
    "                        'N_train':    subset.shape[0],\n",
    "                        'y':          y.tolist()}\n",
    "        \n",
    "        #appending a shallow copy 'samples' of sample - doesn't create copies of the objects it references to\n",
    "        samples.append(sample.copy())\n",
    "\n",
    "    # combine different materials into a single dict/json for stan\n",
    "    combined = samples[0]           #combined is assigned the value of the first element (samples[0]) from the list samples\n",
    "    combined['material_ids'] = np.ones(combined['N_samples'],dtype=int).tolist() #creates a list of ones with a length determined by the value of 'N_samples'\n",
    "    s_max = combined['N_samples']\n",
    "    h_max = combined['N_holders']\n",
    "    l_max = combined['N_locations']\n",
    "    t_max = combined['N_ids']\n",
    "    m_max = 1\n",
    "\n",
    "    for s in samples[1:]:       #code iterates over all elements in the samples list except the first one\n",
    "        #print(s['N'])\n",
    "        combined['N'] += s['N'] #accesses the value associated with the key 'N' in the dictionary combined and incrementing it by the value associated with the key 'N' in each dictionary s in the samples list\n",
    "        combined['N_samples'] += s['N_samples']\n",
    "        combined['N_holders'] += s['N_holders']\n",
    "        combined['N_locations'] += s['N_locations']\n",
    "        combined['N_ids'] += s['N_ids']\n",
    "        combined['sample_ids'].extend((np.array(s['sample_ids'])+s_max).tolist())\n",
    "        combined['holder_ids'].extend((np.array(s['holder_ids'])+h_max).tolist())\n",
    "        combined['location_ids'].extend((np.array(s['location_ids'])+l_max).tolist())\n",
    "        combined['track_ids'].extend((np.array(s['track_ids'])+t_max).tolist())\n",
    "        combined['y'].extend(s['y'])\n",
    "        combined['material_ids'].extend((np.ones(s['N_samples'],dtype=int)+m_max).tolist())\n",
    "\n",
    "        s_max += s['N_samples']\n",
    "        h_max += s['N_holders']\n",
    "        l_max += s['N_locations']\n",
    "        t_max += s['N_ids']\n",
    "        m_max += 1\n",
    "\n",
    "    combined['train_ids'] = ((np.arange(combined['N'])+1).astype(int)).tolist()\n",
    "    combined['N_train'] = len(combined['train_ids'])\n",
    "    combined['sample_ids'] = combined['sample_ids']\n",
    "    combined['holder_ids'] = combined['holder_ids']\n",
    "    combined['location_ids'] = combined['location_ids']\n",
    "    combined['track_ids'] = combined['track_ids']\n",
    "    combined['material_ids'] = np.array(combined['material_ids']).astype(int).tolist()\n",
    "    combined['N_materials'] = len(samples)\n",
    "\n",
    "    import json\n",
    "\n",
    "    #crating path to which the dictionary is saved in json format\n",
    "    \n",
    "    file_path = os.path.join(\"C:/Users/srboval1/IPN15\", f'IPN15_dict_{measurement}.json')\n",
    "\n",
    "    # Writing the dictionary to a JSON file\n",
    "    with open(file_path, \"w\") as json_file:\n",
    "        json.dump(combined, json_file)\n",
    "\n",
    "    # Stan model = math of the Bayesian model\n",
    "    # function CmdStanModel is used to compile the file with Stan model, e.g. converting it into a format that can be executed by the CmdStan command-line tool.\n",
    "    stan_model_path = 'C:/Users/srboval1/InstruProject/microrheology/models/model_Linda.stan'\n",
    "    model = CmdStanModel(stan_file=stan_model_path)\n",
    "    \n",
    "\n",
    "    # Bayes model: all the available data is used to estimate the parameters of the model\n",
    "    # and the posterior distribution which is then used for inference and analysis\n",
    "\n",
    "\n",
    "    #Using sample method of CmdStanModel\n",
    "    # sampling (all) data from the file path by Hamiltonian Monte Carlo (HMC) algorithm\n",
    "    # to generate samples from the posterior distribution of the model parameters\n",
    "    # high adaptation value means adaptation phase is not too aggressive\n",
    "    # maximum depth of the binary tree used by the algorithm - controls the complexity of the trajectories explored by the sampler\n",
    "    fit = model.sample(data=file_path,\n",
    "                            adapt_delta=0.9999999999999999,max_treedepth=20,iter_sampling=2000)\n",
    "    if diagnose:\n",
    "        print(fit.diagnose())\n",
    "\n",
    "    #converting the sampled data from fit into an ArviZ InferenceData object\n",
    "    #ArviZ (az) library allows exploratory analysis of Bayesian models\n",
    "    #object is created from the CmdStanPy object and contains posterior samples, posterior predictive samples, log likelihood, and observed data.\n",
    "    az_data = az.from_cmdstanpy(posterior=fit,\n",
    "                                posterior_predictive='y_hat',\n",
    "                                log_likelihood='log_likelihood',\n",
    "                                observed_data={'y':combined['y']})\n",
    "    \n",
    "    #returns the sampled data obtained from the Bayesian model's sampling output in a pandas DataFrame format in a dictionary\n",
    "    #basic data manipulation and analysis using pandas functionalities\n",
    "    out[measurement] = fit.draws_pd()\n",
    "\n",
    "    #additional functionalities specific to Bayesian analysis\n",
    "    #posterior predictive checks, convergence diagnostics, and visualization tools, through the ArviZ library.\n",
    "    az_out[measurement] = az_data\n",
    "    \n",
    "    return out,az_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\srboval1\\AppData\\Local\\Temp\\ipykernel_30372\\2330346121.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['sample_num'] = sample_num\n",
      "C:\\Users\\srboval1\\AppData\\Local\\Temp\\ipykernel_30372\\2330346121.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subset['sample_num']=subset['sample_num'].astype(int) #convertin sample numbers to integers\n",
      "C:\\Users\\srboval1\\AppData\\Local\\Temp\\ipykernel_30372\\2330346121.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['sample_num'] = sample_num\n",
      "C:\\Users\\srboval1\\AppData\\Local\\Temp\\ipykernel_30372\\2330346121.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subset['sample_num']=subset['sample_num'].astype(int) #convertin sample numbers to integers\n",
      "C:\\Users\\srboval1\\AppData\\Local\\Temp\\ipykernel_30372\\2330346121.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['sample_num'] = sample_num\n",
      "C:\\Users\\srboval1\\AppData\\Local\\Temp\\ipykernel_30372\\2330346121.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subset['sample_num']=subset['sample_num'].astype(int) #convertin sample numbers to integers\n",
      "C:\\Users\\srboval1\\AppData\\Local\\Temp\\ipykernel_30372\\2330346121.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['sample_num'] = sample_num\n",
      "C:\\Users\\srboval1\\AppData\\Local\\Temp\\ipykernel_30372\\2330346121.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subset['sample_num']=subset['sample_num'].astype(int) #convertin sample numbers to integers\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to get source info for Stan model 'C:\\Users\\srboval1\\InstruProject\\microrheology\\models\\model_Linda.stan'. Console:\nSyntax error in 'C:\\Users\\srboval1\\InstruProject\\microrheology\\models\\model_Linda.stan', line 9, column 18 to column 19, parsing error:\n   -------------------------------------------------\n     7:    int N_materials;\n     8:    int N_ids;\n     9:    int material_ids[N_samples];\n                           ^\n    10:    int sample_ids[N_holders];\n    11:    int holder_ids[N_locations];\n   -------------------------------------------------\n\n\";\" expected after variable declaration.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m hh,az_out \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessing\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_Linda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[25], line 145\u001b[0m, in \u001b[0;36mpreprocessing\u001b[1;34m(model_name, diagnose)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# Stan model = math of the Bayesian model\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# function CmdStanModel is used to compile the file with Stan model, e.g. converting it into a format that can be executed by the CmdStan command-line tool.\u001b[39;00m\n\u001b[0;32m    144\u001b[0m stan_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/srboval1/InstruProject/microrheology/models/model_Linda.stan\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 145\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mCmdStanModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstan_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstan_model_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;66;03m# Bayes model: all the available data is used to estimate the parameters of the model\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;66;03m# and the posterior distribution which is then used for inference and analysis\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;66;03m# high adaptation value means adaptation phase is not too aggressive\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;66;03m# maximum depth of the binary tree used by the algorithm - controls the complexity of the trajectories explored by the sampler\u001b[39;00m\n\u001b[0;32m    157\u001b[0m fit \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39msample(data\u001b[38;5;241m=\u001b[39mfile_path,\n\u001b[0;32m    158\u001b[0m                         adapt_delta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9999999999999999\u001b[39m,max_treedepth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,iter_sampling\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\srboval1\\AppData\\Local\\anaconda3\\envs\\stan\\Lib\\site-packages\\cmdstanpy\\model.py:212\u001b[0m, in \u001b[0;36mCmdStanModel.__init__\u001b[1;34m(self, model_name, stan_file, exe_file, force_compile, stanc_options, cpp_options, user_header, compile)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cmdstan_version_before(\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m27\u001b[39m\n\u001b[0;32m    210\u001b[0m ):  \u001b[38;5;66;03m# unknown end of version range\u001b[39;00m\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 212\u001b[0m         model_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msrc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparameters\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_info:\n\u001b[0;32m    214\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fixed_param \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(model_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparameters\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\srboval1\\AppData\\Local\\anaconda3\\envs\\stan\\Lib\\site-packages\\cmdstanpy\\model.py:318\u001b[0m, in \u001b[0;36mCmdStanModel.src_info\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstan_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m cmdstan_version_before(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m27\u001b[39m):\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {}\n\u001b[1;32m--> 318\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msrc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstan_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compiler_options\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\srboval1\\AppData\\Local\\anaconda3\\envs\\stan\\Lib\\site-packages\\cmdstanpy\\compilation.py:361\u001b[0m, in \u001b[0;36msrc_info\u001b[1;34m(stan_file, compiler_options)\u001b[0m\n\u001b[0;32m    359\u001b[0m proc \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mrun(cmd, capture_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, check\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mreturncode:\n\u001b[1;32m--> 361\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    362\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to get source info for Stan model \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    363\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstan_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Console:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mproc\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    364\u001b[0m     )\n\u001b[0;32m    365\u001b[0m result: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(proc\u001b[38;5;241m.\u001b[39mstdout)\n\u001b[0;32m    366\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to get source info for Stan model 'C:\\Users\\srboval1\\InstruProject\\microrheology\\models\\model_Linda.stan'. Console:\nSyntax error in 'C:\\Users\\srboval1\\InstruProject\\microrheology\\models\\model_Linda.stan', line 9, column 18 to column 19, parsing error:\n   -------------------------------------------------\n     7:    int N_materials;\n     8:    int N_ids;\n     9:    int material_ids[N_samples];\n                           ^\n    10:    int sample_ids[N_holders];\n    11:    int holder_ids[N_locations];\n   -------------------------------------------------\n\n\";\" expected after variable declaration.\n"
     ]
    }
   ],
   "source": [
    "hh,az_out = preprocessing(\"model_Linda\", False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
