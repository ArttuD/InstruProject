{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data parser\n",
    "\n",
    "Find and download all microrheology data and format dictionary for stan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:/Users/srboval1/IPN15/collagen\\\\211029_0_10_plain\\\\results\\\\summary_ref_level.csv']\n",
      "       day  frequency concentration      type  sample  holder  location  \\\n",
      "0   211029       0.05           0/2  collagen      10      10        10   \n",
      "1   211029       0.05           0/2  collagen      10      10        10   \n",
      "2   211029       0.05           0/2  collagen      10      10        10   \n",
      "3   211029       0.05           0/2  collagen      10      10        10   \n",
      "4   211029       0.05           0/2  collagen      10      10        10   \n",
      "5   211029       0.05           0/2  collagen      10      10        10   \n",
      "6   211029       0.05           0/2  collagen      10      10        10   \n",
      "7   211029       0.05           0/2  collagen      10      10        10   \n",
      "8   211029       0.05           0/2  collagen      10      10        10   \n",
      "9   211029       0.05           0/2  collagen      10      10        10   \n",
      "10  211029       0.05           0/2  collagen      10      10        10   \n",
      "11  211029       0.05           0/2  collagen      10      10        10   \n",
      "12  211029       0.05           0/2  collagen      10      10        10   \n",
      "13  211029       0.05           0/2  collagen      10      10        10   \n",
      "14  211029       0.05           0/2  collagen      10      10        10   \n",
      "15  211029       0.05           0/2  collagen      10      10        10   \n",
      "16  211029       0.05           0/2  collagen      10      10        10   \n",
      "17  211029       0.05           0/2  collagen      10      10        10   \n",
      "18  211029       0.05           0/2  collagen      10      10        10   \n",
      "19  211029       0.05           0/2  collagen      10      10        10   \n",
      "20  211029       0.05           0/2  collagen      10      10        10   \n",
      "21  211029       0.05           0/2  collagen      10      10        10   \n",
      "22  211029       0.05           0/2  collagen      10      10        10   \n",
      "23  211029       0.05           0/2  collagen      10      10        10   \n",
      "24  211029       0.05           0/2  collagen      10      10        10   \n",
      "25  211029       0.05           0/2  collagen      10      10        10   \n",
      "26  211029       0.05           0/2  collagen      10      10        10   \n",
      "27  211029       0.05           0/2  collagen      10      10        10   \n",
      "28  211029       0.05           0/2  collagen      10      10        10   \n",
      "29  211029       0.05           0/2  collagen      10      10        10   \n",
      "30  211029       0.05           0/2  collagen      10      10        10   \n",
      "31  211029       0.05           0/2  collagen      10      10        10   \n",
      "32  211029       0.05           0/2  collagen      10      10        10   \n",
      "33  211029       0.05           0/2  collagen      10      10        10   \n",
      "34  211029       0.05           0/2  collagen      10      10        10   \n",
      "35  211029       0.05           0/2  collagen      10      10        10   \n",
      "36  211029       0.05           0/2  collagen      10      10        10   \n",
      "37  211029       0.05           0/2  collagen      10      10        10   \n",
      "38  211029       0.05           0/2  collagen      10      10        10   \n",
      "39  211029       0.05           0/2  collagen      10      10        10   \n",
      "40  211029       0.05           0/2  collagen      10      10        10   \n",
      "41  211029       0.05           0/2  collagen      10      10        10   \n",
      "42  211029       0.05           0/2  collagen      10      10        10   \n",
      "43  211029       0.05           0/2  collagen      10      10        10   \n",
      "44  211029       0.05           0/2  collagen      10      10        10   \n",
      "45  211029       0.05           0/2  collagen      10      10        10   \n",
      "\n",
      "    repeat  track_id  reference_id  ...   inv.rmse  shift_(s)   a_error  \\\n",
      "0        1         0             1  ...  44.495555      1.309  0.000861   \n",
      "1        1         0             3  ...  37.208188      1.309  0.001030   \n",
      "2        1         0             5  ...  21.728560      1.309  0.001761   \n",
      "3        1         0             6  ...  37.915824      1.309  0.001011   \n",
      "4        1         0             7  ...  23.434909      1.309  0.001629   \n",
      "5        1         1             1  ...  49.004993      1.309  0.000782   \n",
      "6        1         1             6  ...  51.913253      1.309  0.000738   \n",
      "7        1         1             7  ...  25.836028      1.309  0.001476   \n",
      "8        1         2             5  ...  24.961016      1.309  0.001519   \n",
      "9        1         3             1  ...  52.756168      1.309  0.000727   \n",
      "10       1         3             2  ...  48.744796      1.309  0.000786   \n",
      "11       1         3             3  ...  36.356776      1.309  0.001053   \n",
      "12       1         3             4  ...  39.829494      1.309  0.000962   \n",
      "13       1         3             5  ...  22.211639      1.309  0.001719   \n",
      "14       1         4             2  ...  54.726781      1.309  0.000699   \n",
      "15       1         4             3  ...  38.462222      1.309  0.000994   \n",
      "16       1         4             4  ...  41.667544      1.309  0.000918   \n",
      "17       1         4             5  ...  22.320724      1.309  0.001709   \n",
      "18       1         4             6  ...  40.756760      1.309  0.000937   \n",
      "19       1         4             7  ...  26.847002      1.309  0.001417   \n",
      "20       2         0             0  ...  32.892648      2.410  0.001165   \n",
      "21       2         0             1  ...  28.339470      2.410  0.001353   \n",
      "22       2         0             4  ...  29.402268      2.410  0.001303   \n",
      "23       2         0             5  ...  25.140853      2.410  0.001524   \n",
      "24       2         0             6  ...  29.779586      2.410  0.001286   \n",
      "25       2         0             8  ...  30.902547      2.410  0.001239   \n",
      "26       2         1             5  ...  29.450352      2.410  0.001299   \n",
      "27       2         1             6  ...  41.997785      2.410  0.000910   \n",
      "28       2         1             8  ...  41.021128      2.410  0.000933   \n",
      "29       2         2             2  ...  64.337750      2.410  0.000596   \n",
      "30       2         2             4  ...  27.398898      2.410  0.001377   \n",
      "31       2         2             5  ...  32.452136      2.410  0.001167   \n",
      "32       2         2             6  ...  48.693307      2.410  0.000774   \n",
      "33       2         2             8  ...  50.691243      2.410  0.000751   \n",
      "34       2         3             1  ...  44.577815      2.410  0.000860   \n",
      "35       2         3             3  ...  34.708681      2.410  0.001104   \n",
      "36       2         3             4  ...  25.331664      2.410  0.001507   \n",
      "37       2         3             5  ...  26.186730      2.410  0.001459   \n",
      "38       2         4             2  ...  54.919864      2.410  0.000697   \n",
      "39       2         4             3  ...  50.436345      2.410  0.000758   \n",
      "40       2         4             4  ...  27.702730      2.410  0.001375   \n",
      "41       2         4             5  ...  29.727098      2.410  0.001282   \n",
      "42       2         4             6  ...  45.568011      2.410  0.000836   \n",
      "43       2         5             4  ...  22.595912      2.410  0.001696   \n",
      "44       2         5             5  ...  21.733997      2.410  0.001763   \n",
      "45       2         5             6  ...  28.186573      2.410  0.001360   \n",
      "\n",
      "    phi_error   c_error   d_error           x           y  phi_(deg)   tan_phi  \n",
      "0    0.017400  0.001181  0.000068  117.715841  110.229112  -0.450000 -0.007854  \n",
      "1    0.011001  0.001413  0.000081  117.715841  110.229112  -0.450000 -0.007854  \n",
      "2    0.013866  0.002419  0.000138  117.715841  110.229112  11.792910  0.208782  \n",
      "3    0.013273  0.001386  0.000079  117.715841  110.229112   3.026588  0.052873  \n",
      "4    0.026791  0.002243  0.000128  117.715841  110.229112  18.133188  0.327492  \n",
      "5    0.012614  0.001073  0.000061  190.013169   94.912710  -0.450000 -0.007854  \n",
      "6    0.008256  0.001013  0.000058  190.013169   94.912710   7.994537  0.140444  \n",
      "7    0.019636  0.002035  0.000116  190.013169   94.912710  21.223533  0.388347  \n",
      "8    0.016772  0.002106  0.000120  254.321836   14.604261  31.893159  0.622280  \n",
      "9    0.010634  0.000996  0.000057   18.750112   29.627337  -0.154135 -0.002690  \n",
      "10   0.008538  0.001078  0.000062   18.750112   29.627337   7.062872  0.123899  \n",
      "11   0.009264  0.001446  0.000083   18.750112   29.627337   8.717530  0.153335  \n",
      "12   0.008700  0.001320  0.000075   18.750112   29.627337   5.557594  0.097304  \n",
      "13   0.011467  0.002367  0.000135   18.750112   29.627337  17.968400  0.324310  \n",
      "14   0.004779  0.000961  0.000055   50.935409  202.137537  13.637598  0.242620  \n",
      "15   0.005917  0.001367  0.000078   50.935409  202.137537  13.903213  0.247534  \n",
      "16   0.005585  0.001262  0.000072   50.935409  202.137537  11.900446  0.210741  \n",
      "17   0.008328  0.002355  0.000135   50.935409  202.137537  19.722356  0.358492  \n",
      "18   0.006133  0.001290  0.000074   50.935409  202.137537  18.121908  0.327274  \n",
      "19   0.010057  0.001958  0.000112   50.935409  202.137537  26.092684  0.489737  \n",
      "20   0.048346  0.001598  0.000091  123.502855  105.770632  -0.450000 -0.007854  \n",
      "21   0.026148  0.001855  0.000106  123.502855  105.770632  -0.450000 -0.007854  \n",
      "22   0.013985  0.001788  0.000102  123.502855  105.770632   7.843666  0.137759  \n",
      "23   0.015634  0.002091  0.000120  123.502855  105.770632   5.533236  0.096875  \n",
      "24   0.014159  0.001765  0.000101  123.502855  105.770632   8.645226  0.152043  \n",
      "25   0.008502  0.001701  0.000097  123.502855  105.770632   7.753871  0.136163  \n",
      "26   0.015120  0.001785  0.000102  196.122856   90.259944  12.869805  0.228476  \n",
      "27   0.011385  0.001252  0.000072  196.122856   90.259944  16.971361  0.305184  \n",
      "28   0.006940  0.001281  0.000073  196.122856   90.259944  12.631569  0.224105  \n",
      "29   0.018608  0.000817  0.000047  261.522473    9.937944  -0.450000 -0.007854  \n",
      "30   0.024197  0.001919  0.000110  261.522473    9.937944  38.565235  0.797297  \n",
      "31   0.019739  0.001620  0.000093  261.522473    9.937944  33.341375  0.657911  \n",
      "32   0.013943  0.001080  0.000062  261.522473    9.937944  40.833197  0.864188  \n",
      "33   0.007088  0.001037  0.000059  261.522473    9.937944  23.943678  0.444051  \n",
      "34   0.017406  0.001179  0.000067   24.218383   25.782667   0.457715  0.007989  \n",
      "35   0.016702  0.001514  0.000087   24.218383   25.782667   2.006030  0.035026  \n",
      "36   0.015972  0.002075  0.000119   24.218383   25.782667  18.506627  0.334724  \n",
      "37   0.014886  0.002007  0.000115   24.218383   25.782667  15.811024  0.283179  \n",
      "38   0.005077  0.000957  0.000055   56.765696  197.243496  10.747909  0.189818  \n",
      "39   0.005853  0.001042  0.000060   56.765696  197.243496  16.262028  0.291701  \n",
      "40   0.008571  0.001897  0.000109   56.765696  197.243496  23.334413  0.431380  \n",
      "41   0.007834  0.001768  0.000101   56.765696  197.243496  21.635892  0.396653  \n",
      "42   0.005274  0.001154  0.000066   56.765696  197.243496  23.992070  0.445063  \n",
      "43   0.009525  0.002326  0.000133   40.206490  194.059362   3.957506  0.069182  \n",
      "44   0.009659  0.002419  0.000138   40.206490  194.059362   2.815127  0.049173  \n",
      "45   0.007740  0.001865  0.000107   40.206490  194.059362   4.320539  0.075551  \n",
      "\n",
      "[46 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "# Download data and make a mega .csv fiel\n",
    "\n",
    "paths=[]\n",
    "paths = glob.glob(os.path.join('C:/Users/srboval1/IPN15/collagen/*','**','summary_ref_level.csv'))\n",
    "print(paths)\n",
    "concentration = '0/2'\n",
    "type ='collagen'\n",
    "\n",
    "a = []\n",
    "for i in paths:\n",
    "    path1 = os.path.split(os.path.split(i)[0])[0]\n",
    "    path1.replace(\"/\",\"\\\\\")\n",
    "    splitted= path1.split('\\\\')[-1]\n",
    "    tmp = pd.read_csv(i)\n",
    "    tmp['coating_type'] = splitted.split('_')[-1]\n",
    "    tmp['size'] = splitted.split('_')[-2]\n",
    "    tmp['day'] = splitted.split('_')[-4]\n",
    "    a.append(tmp)\n",
    "\n",
    "a_concantenated=[]\n",
    "a_concantenated = pd.concat(a)\n",
    "a_concantenated['radius_(m)'] *= 1e6\n",
    "a_concantenated = a_concantenated.rename(columns={'radius_(m)':'radius_(um)'})\n",
    "a_concantenated['concentration'] = concentration\n",
    "a_concantenated['type'] = type\n",
    "a_concantenated['frequency'] = 0.05\n",
    "a_concantenated = a_concantenated.reindex(columns=['day','frequency','concentration','type','sample','holder','location','repeat','track_id','reference_id','distance(um)','Cov_Sum','a_(um)','phi_(rad)','c','d','G_abs','radius_(m)','r2','rmse','inv.rmse','shift_(s)','a_error','phi_error','c_error','d_error','x','y','phi_(deg)','tan_phi'])\n",
    "print(a_concantenated)\n",
    "#%%\n",
    "if os.path.exists('C:/Users/srboval1/IPN15/IPN15.csv'):\n",
    "    a_new = pd.DataFrame(a_concantenated)\n",
    "    a_new.to_csv('C:/Users/srboval1/IPN15/IPN15.csv', mode='a', index=False, header=False)\n",
    "else:\n",
    "    a_concantenated.to_csv(\"C:/Users/srboval1/IPN15/IPN15.csv\", index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3880.8730321837993\n",
      "10.763319073259654\n",
      "['G_abs', 'phi_(rad)'] \n",
      " ['IPN soft' 'IPN stiff' 'collagen']\n"
     ]
    }
   ],
   "source": [
    "data_all = pd.read_csv('C:/Users/srboval1/IPN15/IPN15.csv')\n",
    "\n",
    "#calculating G std for each material typeand then choosing the highest one of all\n",
    "G_std = np.max(data_all[['type','day','holder','location','track_id',\"G_abs\", \"phi_(deg)\"]].groupby([\"type\"]).std()[\"G_abs\"].values)\n",
    "print(G_std)\n",
    "\n",
    "#calculating G std for each material type and then choosing the highest one of all\n",
    "phi_std = np.max(data_all[['type','day','holder','location','track_id',\"G_abs\", \"phi_(deg)\"]].groupby([\"type\"]).std()[\"phi_(deg)\"].values)\n",
    "print(phi_std)\n",
    "\n",
    "\n",
    "full_path = os.path.split(os.getcwd())[0]\n",
    "\n",
    "print([\"G_abs\", \"phi_(rad)\"], \"\\n\",data_all[\"type\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\srboval1\\AppData\\Local\\Temp\\ipykernel_17808\\1473737976.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['sample_num'] = sample_num\n",
      "C:\\Users\\srboval1\\AppData\\Local\\Temp\\ipykernel_17808\\1473737976.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subset['sample_num']=subset['sample_num'].astype(int) #convertin sample numbers to integers\n",
      "C:\\Users\\srboval1\\AppData\\Local\\Temp\\ipykernel_17808\\1473737976.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['sample_num'] = sample_num\n",
      "C:\\Users\\srboval1\\AppData\\Local\\Temp\\ipykernel_17808\\1473737976.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subset['sample_num']=subset['sample_num'].astype(int) #convertin sample numbers to integers\n",
      "C:\\Users\\srboval1\\AppData\\Local\\Temp\\ipykernel_17808\\1473737976.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['sample_num'] = sample_num\n",
      "C:\\Users\\srboval1\\AppData\\Local\\Temp\\ipykernel_17808\\1473737976.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subset['sample_num']=subset['sample_num'].astype(int) #convertin sample numbers to integers\n",
      "C:\\Users\\srboval1\\AppData\\Local\\Temp\\ipykernel_17808\\1473737976.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['sample_num'] = sample_num\n",
      "C:\\Users\\srboval1\\AppData\\Local\\Temp\\ipykernel_17808\\1473737976.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subset['sample_num']=subset['sample_num'].astype(int) #convertin sample numbers to integers\n",
      "C:\\Users\\srboval1\\AppData\\Local\\Temp\\ipykernel_17808\\1473737976.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['sample_num'] = sample_num\n",
      "C:\\Users\\srboval1\\AppData\\Local\\Temp\\ipykernel_17808\\1473737976.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subset['sample_num']=subset['sample_num'].astype(int) #convertin sample numbers to integers\n",
      "C:\\Users\\srboval1\\AppData\\Local\\Temp\\ipykernel_17808\\1473737976.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['sample_num'] = sample_num\n",
      "C:\\Users\\srboval1\\AppData\\Local\\Temp\\ipykernel_17808\\1473737976.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subset['sample_num']=subset['sample_num'].astype(int) #convertin sample numbers to integers\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "data_all = pd.read_csv('C:/Users/srboval1/IPN15/IPN15.csv')\n",
    "\n",
    "#%%\n",
    "\n",
    "#process mechanical properties separately, \n",
    "#for the final analysis it measures either G or phi\n",
    "for measurement in ['G_abs','phi_(deg)']:\n",
    "    samples = []\n",
    "\n",
    "    #combine information of all materials\n",
    "    #looping through each material one by one\n",
    "    for material in data_all[\"type\"].unique():\n",
    "        data_type = material\n",
    "        data = data_all[data_all['type'] == data_type]\n",
    "\n",
    "        #initializing empty list that will contain corresponding numbers for each sample\n",
    "        sample_num = []\n",
    "        \n",
    "        prev = None\n",
    "        idx = 1\n",
    "        #assuming one sample per day, adding sample ids to the list in sample_num\n",
    "        #iterating over each element in the 'day' column of data and creating the sample_num list\n",
    "        for i in data['day']:\n",
    "            if prev is None:\n",
    "                prev = i\n",
    "            else: #changed from Ossi's code 'if' - to have distinct sample ids\n",
    "                idx += 1\n",
    "                prev = i\n",
    "            sample_num.append(idx)      \n",
    "        data['sample_num'] = sample_num\n",
    "        subset = data [['sample_num','holder','location', 'track_id', measurement]]\n",
    "        subset['sample_num']=subset['sample_num'].astype(int) #convertin sample numbers to integers\n",
    "        \n",
    "        sample_ids=[]\n",
    "        location_ids=[]\n",
    "        holder_ids=[]\n",
    "        track_ids=[]\n",
    "\n",
    "        n_loc = 0       #initial number of ids in locations\n",
    "        n_hold = 0      #initial locations in holders\n",
    "        n_samp = 0    #initial holders in samples\n",
    "\n",
    "    #change into consecutive numbering\n",
    "        for i in subset['sample_num'].unique():             \n",
    "            sub_sample = subset[subset['sample_num']==i]        #new df containing only the rows (measurements) with that unique sample_num i\n",
    "            sample_ids.extend([int(i)]*sub_sample['holder'].unique().shape[0])\n",
    "            for jj,j in enumerate(sub_sample['holder'].unique()):\n",
    "                sub = sub_sample[sub_sample['holder']==j]\n",
    "                holder_ids.extend([int(jj+1)+n_samp]*sub['location'].unique().shape[0]) #list that will eventually be listing all holder ids for each location - add identity number of holder times the number of locations (unique holder numbers)\n",
    "                for kk,k in enumerate(sub['location'].unique()):\n",
    "                    sub2 = sub_sample[sub_sample['location']==j]\n",
    "                    location_ids.extend([int(kk+1)+n_hold]*sub['track_id'].unique().shape[0])\n",
    "                    for zz, z in enumerate(sub2['track_id'].unique()):\n",
    "                        sub3 = sub2[sub2['track_id']==z]\n",
    "                        track_ids.extend([int(zz+1)+n_loc]*sub3.shape[0]) #shape[0] size of the first dimension = row\n",
    "                    n_loc += int(sub2['track_id'].unique().shape[0])    #number of tracks ids that have been looped through already; updated with each location\n",
    "                n_hold += int(sub['location'].unique().shape[0])    #number of locations that have been looped through; used to add up in the next holder\n",
    "            n_samp += int(sub_sample['holder'].unique().shape[0])   #number of holders that have been looped through; used to add up in the next sample\n",
    "\n",
    "    # normalize mean to zero and with common std \n",
    "    y_raw = subset[measurement].values  #retrieves values of the column labelled measurement from df subset and converts into array by .value\n",
    "    if measurement == 'G_abs':          #compares measurement to a fixed string 'G_abs' variable\n",
    "        y = y_raw/G_std                 #each value scaled by std\n",
    "    else:\n",
    "        y = y_raw/phi_std\n",
    "\n",
    "    #defining dictionary; curly brackets and specified key-value pairs separated by : each key corresponds to a specific attribute }\n",
    "    sample = {    'N':subset.shape[0],\n",
    "                    'N_samples':  int(np.max(sample_ids)),\n",
    "                    'N_holders':  int(np.max(holder_ids)),\n",
    "                    'N_locations':int(np.max(location_ids)),\n",
    "                    'N_ids':      int(np.max(track_ids)),\n",
    "                    'sample_ids': sample_ids,\n",
    "                    'holder_ids': holder_ids,\n",
    "                    'location_ids':location_ids,\n",
    "                    'track_ids':  track_ids,\n",
    "                    'train_ids':  (np.arange(len(track_ids))+1).tolist(),\n",
    "                    'N_train':    subset.shape[0],\n",
    "                    'y':          y.tolist()}\n",
    "    \n",
    "    #appending a shallow copy 'samples' of sample - doesn't create copies of the objects it references to\n",
    "    samples.append(sample.copy())\n",
    "\n",
    "# combine different materials into a single dict/json for stan\n",
    "combined = samples[0]           #combined is assigned the value of the first element (samples[0]) from the list samples\n",
    "combined['material_ids'] = np.ones(combined['N_samples'],dtype=int).tolist() #creates a list of ones with a length determined by the value of 'N_samples'\n",
    "s_max = combined['N_samples']\n",
    "h_max = combined['N_holders']\n",
    "l_max = combined['N_locations']\n",
    "t_max = combined['N_ids']\n",
    "m_max = 1\n",
    "\n",
    "for s in samples[1:]:       #code iterates over all elements in the samples list except the first one\n",
    "    #print(s['N'])\n",
    "    combined['N'] += s['N'] #accesses the value associated with the key 'N' in the dictionary combined and incrementing it by the value associated with the key 'N' in each dictionary s in the samples list\n",
    "    combined['N_samples'] += s['N_samples']\n",
    "    combined['N_holders'] += s['N_holders']\n",
    "    combined['N_locations'] += s['N_locations']\n",
    "    combined['N_ids'] += s['N_ids']\n",
    "    combined['sample_ids'].extend((np.array(s['sample_ids'])+s_max).tolist())\n",
    "    combined['holder_ids'].extend((np.array(s['holder_ids'])+h_max).tolist())\n",
    "    combined['location_ids'].extend((np.array(s['location_ids'])+l_max).tolist())\n",
    "    combined['track_ids'].extend((np.array(s['track_ids'])+t_max).tolist())\n",
    "    combined['y'].extend(s['y'])\n",
    "    combined['material_ids'].extend((np.ones(s['N_samples'],dtype=int)+m_max).tolist())\n",
    "\n",
    "    s_max += s['N_samples']\n",
    "    h_max += s['N_holders']\n",
    "    l_max += s['N_locations']\n",
    "    t_max += s['N_ids']\n",
    "    m_max += 1\n",
    "\n",
    "combined['train_ids'] = ((np.arange(combined['N'])+1).astype(int)).tolist()\n",
    "combined['N_train'] = len(combined['train_ids'])\n",
    "combined['sample_ids'] = combined['sample_ids']\n",
    "combined['holder_ids'] = combined['holder_ids']\n",
    "combined['location_ids'] = combined['location_ids']\n",
    "combined['track_ids'] = combined['track_ids']\n",
    "combined['material_ids'] = np.array(combined['material_ids']).astype(int).tolist()\n",
    "combined['material_ids'] = np.array(combined['material_ids']).astype(int).tolist()\n",
    "combined['N_materials'] = len(samples)\n",
    "\n",
    "import json\n",
    "\n",
    "#crating path to save the dictionary in json format\n",
    "file_path = \"C:/Users/srboval1/IPN15/IPN15_dict.json\"\n",
    "\n",
    "# Writing the dictionary to a JSON file\n",
    "with open(file_path, \"w\") as json_file:\n",
    "    json.dump(combined, json_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
